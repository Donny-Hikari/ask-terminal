chat_terminal:
  endpoint: "openai"
  prompt: "prompts/chat-terminal.mext"
  max_observation_length: 1024  # tokens

  user: "User"
  agent: "Assistant"

text_completion_endpoints:
  ollama:
    server_url: "http://127.0.0.1:11434"
    model: "llama3.1"
  local-llama:
    server_url: "http://127.0.0.1:40080"
    params:
      temperature: 0.75
      top_k: 40
      top_p: 0.9
      n_predict: 4096
      stop:
        - "[INST]"
  openai:
    model: "gpt-3.5-turbo"
  anthropic:
    model: "claude-3-haiku"
